{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель основывалась на данных прогноза погоды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_point = {\n",
    "    3019: ['Киренск', [30028, 30219, 30328, 30337, 30433, 30230]],\n",
    "    3027: ['Витим', [24713, 24726, 24817, 30356, 30471, 30372, 30069, 30253, 30252, 24923]],\n",
    "    3028: ['Пеледуй', [24713, 24726, 24817, 30356, 30471, 30372, 30069, 30253, 30252, 24923]],\n",
    "    3029: ['Крестовский Лесоучасток', [24713, 24726, 24817, 30356, 30471, 30372, 30069, 30253, 30252, 24923]],\n",
    "    3030: ['Ленск', [24713, 24726, 24817, 30356, 30471, 30372, 30069, 30253, 30252, 24923]],\n",
    "    3035: ['Олекминск',[24538,24738,24641,24933,30089,30385,30493,30393,31102,31004,24951,24944]],\n",
    "    3041: ['Покровск', [31137,31026,24967,24966,24641,24643,24661,24671,24763]],\n",
    "    3045: ['Якутск', [31137,31026,24967,24966,24641,24643,24661,24671,24763]],\n",
    "    3230: ['Батамай',[31137,31026,24967,24966,24641,24643,24661,24671,24763]],\n",
    "    3050: ['Сангар',[31137,31026,24967,24966,24641,24643,24661,24671,24763]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Данные/2_track_cp4/test.csv')\n",
    "test['date'] = pd.to_datetime(test['date'], dayfirst=True)\n",
    "test.set_index('date', inplace=True)\n",
    "\n",
    "extra_train = pd.read_csv('Данные/2_track_cp4/extra_train.csv')\n",
    "forecast_meteo_3hours = pd.read_csv('Данные/2_track_cp4/forecast_meteo_3hours.csv')\n",
    "extra_meteo_3hours = pd.read_csv('Данные/2_track_cp4/extra_meteo_3hours.csv')\n",
    "\n",
    "extra_train['date'] = pd.to_datetime(extra_train['date'], dayfirst=True)\n",
    "extra_train.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "train = pd.read_csv('Данные/track_2_package/train.csv')\n",
    "train['date'] = pd.to_datetime(train['date'], dayfirst=True)\n",
    "train.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_col = ['station_id', 'air_temperature', 'precipitation', 'wind_speed_aver', 'wind_direction', 'date_local']\n",
    "col_to_agg = ['air_temperature', 'precipitation', 'wind_speed_aver', 'wind_direction']\n",
    "meteo = extra_meteo_3hours[need_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(train, meteo_aggregate, name_point):\n",
    "    data = pd.concat([\n",
    "        train.loc[(train['station_id']==point)].merge(\n",
    "            meteo_aggregate.loc[name_point[point][1]].groupby('date').agg('mean'), \n",
    "            how='left', \n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        ) for point in name_point.keys()\n",
    "    ])\n",
    "    return data\n",
    "\n",
    "def aggr_mean(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    suffix = '_mean'\n",
    "    series = pd.Series(np.nanmean(data, axis=0)).to_frame().T.copy()\n",
    "    series.columns = data.columns\n",
    "    series = series.add_suffix(suffix)\n",
    "    return series\n",
    "\n",
    "def aggr_min(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    suffix = '_min'\n",
    "    series = pd.Series(np.min(data, axis=0)).to_frame().T.copy()\n",
    "    series.columns = data.columns\n",
    "    series = series.add_suffix(suffix)\n",
    "    return series\n",
    "\n",
    "def aggr_max(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    suffix = '_max'\n",
    "    series = pd.Series(np.max(data, axis=0)).to_frame().T.copy()\n",
    "    series.columns = data.columns\n",
    "    series = series.add_suffix(suffix)\n",
    "    return series\n",
    "    \n",
    "def aggr_std(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    suffix = '_std'\n",
    "    series = pd.Series(np.nanstd(data, axis=0)).to_frame().T.copy()\n",
    "    series.columns = data.columns\n",
    "    series = series.add_suffix(suffix)\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr = [aggr_mean, aggr_min, aggr_max, aggr_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meteo_aggregate_test = extra_meteo_3hours.groupby(['station_id', 'date_local']).apply(lambda x: pd.concat([func(x[col_to_agg]) for func in aggr], axis=1))\n",
    "meteo_aggregate_test['date'] = meteo_aggregate_test.index.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработанный файл с исторической погодой\n",
    "with open('meteo3_hour_agg.pickle', 'rb') as f:\n",
    "    meteo_aggregate_train = pickle.load(f)\n",
    "    \n",
    "meteo_aggregate_train['date'] = meteo_aggregate_train.index.get_level_values(1)\n",
    "meteo_aggregate_train['date'] = pd.to_datetime(meteo_aggregate_train['date'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = merge(train, meteo_aggregate_train, name_point)\n",
    "data_train.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = merge(extra_train, meteo_aggregate_test, name_point)\n",
    "data_test.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train.loc[(data_train['month']==4) | (data_train['month']==5) | (data_train['month']==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(max_depth=4, n_estimators=100, n_jobs=10, subsample=0.91)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_model = [\n",
    "    'air_temperature_mean', 'precipitation_mean', 'wind_speed_aver_mean',  \n",
    "    'air_temperature_min',    'precipitation_min',  'wind_speed_aver_min',\n",
    "    'wind_direction_min',  'air_temperature_max',    'precipitation_max',\n",
    "    'wind_speed_aver_max',   'wind_direction_max',  'air_temperature_std',\n",
    "    'precipitation_std',  'wind_speed_aver_std',   'wind_direction_std',\n",
    "    'wind_direction_mean'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.42 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=10, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.91,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(data_train[columns_model], data_train['delta_stage_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['predict'] = model.predict(data_test[columns_model])\n",
    "data_test['stats'] = np.nan\n",
    "for day in data_test['day'].unique():\n",
    "    for station in data_test['station_id'].unique():\n",
    "        data_test.loc[(data_test['day']==day) & (data_test['station_id']==station), 'stats'] = \\\n",
    "        data_train.loc[(data_train['day']==day) & (data_train['station_id']==station)]['delta_stage_max'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3041_predict:  1266.87\n",
      "3041_stat:  1075.03\n",
      "\n",
      "3230_predict:  891.55\n",
      "3230_stat:  751.33\n",
      "\n",
      "3045_predict:  1015.84\n",
      "3045_stat:  763.09\n",
      "\n",
      "3027_predict:  2696.11\n",
      "3027_stat:  2880.4\n",
      "\n",
      "3029_predict:  2513.65\n",
      "3029_stat:  2612.22\n",
      "\n",
      "3050_predict:  2338.48\n",
      "3050_stat:  1867.19\n",
      "\n",
      "3019_predict:  470.03\n",
      "3019_stat:  478.79\n",
      "\n",
      "3030_predict:  1803.81\n",
      "3030_stat:  2138.14\n",
      "\n",
      "3028_predict:  2489.13\n",
      "3028_stat:  2499.7\n",
      "\n",
      "3035_predict:  2791.01\n",
      "3035_stat:  2600.48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for station in data_test['station_id'].unique():\n",
    "    data = data_test.loc[data_test['station_id']==station]\n",
    "    print(f'{station}_predict: ', round(mean_squared_error(data['delta_stage_max'], data['predict']), 2))\n",
    "    print(f'{station}_stat: ', round(mean_squared_error(data['delta_stage_max'], data['stats']), 2), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump([model, columns_model], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# На сабмит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexs\\pycharmprojects\\env\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice\n",
      "  \n",
      "c:\\users\\alexs\\pycharmprojects\\env\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1665: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "meteo_aggregate_ = forecast_meteo_3hours.groupby(['station_id', 'date_local']).apply(lambda x: pd.concat([func(x[col_to_agg]) for func in aggr], axis=1))\n",
    "meteo_aggregate_['date'] = meteo_aggregate_.index.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = merge(test, meteo_aggregate_, name_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_train, data_test, test_])\n",
    "data_all.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    'air_temperature_mean', 'precipitation_mean', 'wind_speed_aver_mean',  \n",
    "    'air_temperature_min',    'precipitation_min',  'wind_speed_aver_min',\n",
    "    'wind_direction_min',  'air_temperature_max',    'precipitation_max',\n",
    "    'wind_speed_aver_max',   'wind_direction_max',  'air_temperature_std',\n",
    "    'precipitation_std',  'wind_speed_aver_std',   'wind_direction_std', \n",
    "    'wind_direction_mean', 'delta_stage_max', 'day'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как не удолась выгрузить метеоданные можем интерполировать наши признаки для теста\n",
    "data_inter = data_all.groupby(['station_id', 'year']).apply(lambda x: x.sort_index()[columns].interpolate(method='time'))\n",
    "data_inter['station_id'] = data_inter.index.get_level_values(0)\n",
    "data_inter['year'] = data_inter.index.get_level_values(1)\n",
    "data_inter.index = data_inter.index.get_level_values(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in test['day'].unique():\n",
    "    for station in test['station_id'].unique():\n",
    "        test.loc[(test['day']==day) & (test['station_id']==station), 'stats'] = \\\n",
    "        data_train.loc[(data_train['day']==day) & (data_train['station_id']==station)]['delta_stage_max'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inter['date'] = data_inter.index\n",
    "data_inter['predict'] = model.predict(data_inter[columns_model])\n",
    "test['date'] = test.index\n",
    "\n",
    "for day in test.index.unique():\n",
    "    for station in test['station_id'].unique():\n",
    "        test.loc[(test['date']==day) & (test['station_id']==station), 'delta_stage_max'] = \\\n",
    "        data_inter.loc[(data_inter['date']==day) & (data_inter['station_id']==station)]['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['delta_stage_max'] = (test['delta_stage_max']+test['stats'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['year', 'station_id', 'month', 'day', 'date', 'delta_stage_max']].to_csv('Sub_9.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
